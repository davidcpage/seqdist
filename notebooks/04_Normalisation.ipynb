{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidcpage/seqdist/blob/master/notebooks/01_CTC_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation\n",
    "\n",
    "> Partition function calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import torch\n",
    "from seqdist.utils import *\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a test example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_test_example(T, N, n_state, dtype=torch.float):\n",
    "    return torch.rand((T, N, n_state, n_state), device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4032//5\n",
    "N = 128\n",
    "n_state = 8\n",
    "Ms = generate_test_example(T, N, n_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "\n",
    "def _rescale(M):\n",
    "    #T, N, n_state, n_state = M.shape\n",
    "    Z = M.sum((2, 3), keepdim=True) / M.size(3)\n",
    "    logZ = torch.log(Z).sum(0).reshape(-1)\n",
    "    return M / Z, logZ    \n",
    "\n",
    "@torch.jit.script\n",
    "def logZ_py(M, alpha_0):\n",
    "    M, logZ = _rescale(M)\n",
    "    T, N, n_state, _ = M.shape\n",
    "    alpha = alpha_0.unsqueeze(2)\n",
    "    for i, M_t in enumerate(M.unbind(0)):\n",
    "        alpha = M_t.bmm(alpha)\n",
    "        if i % 32 == (T - 1) % 32:\n",
    "            z = alpha.sum(1, keepdim=True)\n",
    "            alpha = alpha/z\n",
    "            logZ += torch.log(z.squeeze())\n",
    "    return logZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report(benchmark_fwd_bwd((lambda M, alpha_0: logZ_fwd(M, alpha_0).mean()), M, alpha_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda/fused_bmv.cu\n",
    "__device__ __forceinline__ FLOAT max2(FLOAT a, FLOAT a1) {\n",
    "    return a > a1 ? a : a1; \n",
    "}\n",
    "\n",
    "__device__ __forceinline__ FLOAT logsumexp2(FLOAT a, FLOAT a1) {\n",
    "    FLOAT maxa = max2(a, a1); \n",
    "    return maxa + log(exp(a-maxa) + exp(a1-maxa));\n",
    "}\n",
    "\n",
    "__device__ __forceinline__ FLOAT add(FLOAT a, FLOAT b) {return a + b;}\n",
    "__device__ __forceinline__ FLOAT mul(FLOAT a, FLOAT b) {return a * b;}\n",
    "\n",
    "extern \"C\" __global__ void fwd(\n",
    "    FLOAT* __restrict__ alpha,\n",
    "    const FLOAT* __restrict__ Ms, \n",
    "    int T, int N, int n_state\n",
    ") {\n",
    "    // Ms is shape (T, N, n_state, n_state)\n",
    "    // alpha is shape (T + 1, N, n_state)\n",
    "    // assumes blockDim = (N, 1, 1) and threadDim = (n_state, 1, 1)\n",
    "\n",
    "    int bx = blockIdx.x, tx = threadIdx.x;\n",
    "    if (tx >= n_state) return;\n",
    "    FLOAT u;\n",
    "    for (int t = 0; t < T; t++) {\n",
    "        int j = (t * N + bx) * n_state;\n",
    "        u = MUL(Ms[(j + tx) * n_state], alpha[j]);\n",
    "        for (int i = 1; i < n_state; i++) {\n",
    "            u = SUM(u, MUL(Ms[(j + tx) * n_state + i], alpha[j + i]));\n",
    "        }\n",
    "        alpha[j + (N * n_state) + tx] = u;\n",
    "        __syncthreads();\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from seqdist.ctc import semiring, Log, Max\n",
    "from functools import partial\n",
    "\n",
    "cupy_funcs = {\n",
    "    (torch.float32, Log): load_cupy_func('cuda/fused_bmv.cu', 'fwd', FLOAT='float', MUL='add', SUM='logsumexp2'),\n",
    "    (torch.float64, Log): load_cupy_func('cuda/fused_bmv.cu', 'fwd', FLOAT='double', MUL='add', SUM='logsumexp2'),\n",
    "    (torch.float32, Max): load_cupy_func('cuda/fused_bmv.cu', 'fwd', FLOAT='float',  MUL='add', SUM='max2'),\n",
    "    (torch.float64, Max): load_cupy_func('cuda/fused_bmv.cu', 'fwd', FLOAT='double', MUL='add', SUM='max2'),\n",
    "}\n",
    "\n",
    "def fused_batch_Mv(Ms, alpha_0, S:semiring=Log):\n",
    "    T, N, n_state, _ = Ms.shape\n",
    "    alpha = Ms.new_empty((T + 1, N, n_state))\n",
    "    alpha[0] = alpha_0\n",
    "    with cp.cuda.Device(Ms.device.index):\n",
    "        cupy_funcs[(Ms.dtype, S)](\n",
    "            grid=(N, 1, 1), \n",
    "            block=(n_state, 1, 1), \n",
    "            args=(alpha.data_ptr(), Ms.contiguous().data_ptr(), T, N, n_state)\n",
    "        )\n",
    "    return alpha\n",
    "\n",
    "def _logz_fwd(ctx, Ms, alpha_0, beta_T, S:semiring=Log):\n",
    "    alpha = fused_batch_Mv(Ms, alpha_0, S)\n",
    "    ctx.save_for_backward(Ms, alpha, beta_T)\n",
    "    return S.sum(S.mul(alpha[-1], beta_T), dim=1)    \n",
    "\n",
    "def _logz_bwd(ctx, g, S:semiring=Log):\n",
    "    Ms, alpha, beta_T = ctx.saved_tensors\n",
    "    T, N, n_state, _ = Ms.shape\n",
    "    beta = fused_batch_Mv(Ms.transpose(2, 3).flip(0), beta_T, S)\n",
    "    Ms_grad = S.mul(S.mul(Ms, alpha[:-1,:,None,:]), (beta[:-1, :, :, None]).flip(0))\n",
    "    Ms_grad = S.dsum(Ms_grad.reshape(T, N, -1), dim=2).reshape(T, N, n_state, n_state)\n",
    "    return Ms_grad * g[None, :, None, None], None, None, None \n",
    "\n",
    "class LogZ(torch.autograd.Function):\n",
    "    forward = staticmethod(_logz_fwd)\n",
    "    backward = staticmethod(_logz_bwd)\n",
    "\n",
    "class LogZViterbi(torch.autograd.Function):\n",
    "    forward = staticmethod(partial(_logz_fwd, S=Max))\n",
    "    backward = staticmethod(partial(_logz_bwd, S=Max))\n",
    "\n",
    "def logz(Ms, alpha_0, beta_T, S:semiring=Log):\n",
    "    if S==Log:\n",
    "        return LogZ.apply(Ms, alpha_0, beta_T)\n",
    "    elif S==Max:\n",
    "        return LogZViterbi.apply(Ms, alpha_0, beta_T)\n",
    "    else: \n",
    "        raise Exception('semiring {} not supported'.format(S))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
