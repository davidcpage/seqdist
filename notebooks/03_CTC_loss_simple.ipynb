{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_CTC_loss_simple.ipynb","provenance":[{"file_id":"12XP_WZ0yozRtpkmILZ8lJg8LYKx5jRbt","timestamp":1594981909636}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/davidcpage/mctc/blob/master/notebooks/01_CTC_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"5eu0cLH33Q09","colab_type":"code","colab":{}},"source":["# default_exp ctc_simple"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lwSCvHH9S_TO","colab_type":"text"},"source":["# CTC loss simple\n","\n","> A simplified CTC loss for decoding lattices with only two options stay/move. This can be used for decoding without collapsing of repeats."]},{"cell_type":"code","metadata":{"id":"3Rsd7qFH3rK7","colab_type":"code","colab":{}},"source":["#export\n","import numpy as np\n","import cupy as cp\n","import torch\n","import torch.nn as nn\n","from collections import namedtuple\n","from mctc.utils import *\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfMZxoqjAzUd","colab_type":"text"},"source":["## Preliminaries"]},{"cell_type":"markdown","metadata":{"id":"f5QhUc8ErIKK","colab_type":"text"},"source":["Generate a test example:"]},{"cell_type":"code","metadata":{"id":"hKNBt8XDa9y6","colab_type":"code","colab":{}},"source":["#export\n","def generate_sample_inputs(T, N, L_min, L_max, device=device):\n","    \"\"\"\n","    Args:\n","        T: number of time steps\n","        N: batch size\n","        L_min, L_max: bounds on target length\n","    \"\"\"\n","    stay_scores = torch.rand(T, N, L_max, device=device, requires_grad=True)\n","    move_scores = torch.rand(T, N, L_max-1, device=device, requires_grad=True)\n","    target_lengths = torch.randint(L_min, L_max+1, (N,), device=device)\n","    return stay_scores, move_scores, target_lengths"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1iRyJwqbWib","colab_type":"code","colab":{}},"source":["sample_inputs = stay_scores, move_scores, target_lengths = generate_sample_inputs(T=800, N=64, L_min=330, L_max=500)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6FN28tVWEmsK","colab_type":"text"},"source":["## Loss implementations"]},{"cell_type":"markdown","metadata":{"id":"FmZa5QcOcEzC","colab_type":"text"},"source":["### 1. Basic pytorch"]},{"cell_type":"markdown","metadata":{"id":"rVkKq1yHvBBK","colab_type":"text"},"source":["Here's a straightforward implementation in pytorch in logspace.\n"]},{"cell_type":"code","metadata":{"id":"FF5IdGiMWNLc","colab_type":"code","colab":{}},"source":["#export\n","from torch.nn.functional import pad\n","from mctc.ctc import Log, semiring\n","\n","def logZ_fwd(stay_scores, move_scores, target_lengths, S=Log):\n","    T, N, L = stay_scores.shape\n","    alpha_0 = stay_scores.new_full((N, L), S.zero); alpha_0[:, 0] = S.one\n","    beta_T = stay_scores.new_full((N, L), S.zero); beta_T[torch.arange(N), target_lengths - 1] = S.one\n","    move_scores = pad(move_scores, (1, 0), value=S.zero)\n","    a = pad(alpha_0, (1, 0), value=S.zero)\n","    for t in range(0, stay_scores.size(0)):\n","        a[:, 1:] = S.sum(torch.stack([\n","            S.mul(stay_scores[t], a[:, 1:]),\n","            S.mul(move_scores[t], a[:, :-1])\n","        ]), dim=0)    \n","    return S.sum(S.mul(a[:, 1:], beta_T), dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nhDtvcY6WyaW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"executionInfo":{"status":"ok","timestamp":1594983794273,"user_tz":-60,"elapsed":688,"user":{"displayName":"david page","photoUrl":"","userId":"15385526310632231424"}},"outputId":"4a9afa08-af61-4af8-ea91-17e85daabc6a"},"source":["res = logZ_fwd(*sample_inputs)\n","res"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([970.6453, 971.6895, 982.3431, 972.2327, 976.6527, 977.2905, 979.2094,\n","        979.4549, 984.2569, 977.5510, 969.8797, 981.0777, 982.6025, 981.8260,\n","        975.4854, 980.4288, 983.0338, 984.4608, 975.9901, 973.8077, 983.5836,\n","        984.6028, 981.6602, 980.5414, 976.8457, 980.7314, 966.9128, 968.4935,\n","        982.5735, 983.4719, 982.3672, 981.2139, 968.8883, 981.0426, 976.7378,\n","        976.1961, 979.2068, 981.1317, 977.9713, 961.0184, 971.9141, 969.5038,\n","        979.3978, 978.5461, 982.9652, 964.3593, 980.6489, 984.6378, 979.3281,\n","        984.9939, 978.5788, 961.8087, 972.8093, 980.8213, 970.1132, 981.3785,\n","        975.8100, 964.0969, 982.7523, 972.8085, 976.3800, 984.9760, 970.7934,\n","        982.0917], device='cuda:0', grad_fn=<LogsumexpBackward>)"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"2SmEuWACD1Gw","colab_type":"code","colab":{}},"source":["#report(benchmark_fwd_bwd((lambda *x: logZ_fwd(*x).sum()), *sample_inputs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eOyepFIsaL_z","colab_type":"text"},"source":["### 2. Pytorch with grad"]},{"cell_type":"code","metadata":{"id":"5tPPCYDgYDj3","colab_type":"code","colab":{}},"source":["#export\n","def _simple_lattice_fwd_bwd(alpha, beta_T, beta_stay, beta_move, stay_scores, move_scores, S=Log):\n","    T = alpha.size(0) - 1\n","    move_scores = pad(move_scores, (1, 1), value=S.zero)\n","    a = pad(alpha[0], (1, 0), value=S.zero)\n","    for t in range(0, T):\n","        a[:, 1:] = S.sum(torch.stack([\n","            S.mul(stay_scores[t], a[:, 1:]),\n","            S.mul(move_scores[t, :, :-1], a[:, :-1])\n","        ]), dim=0)\n","        alpha[t+1] = a[:, 1:]\n","    \n","    b = pad(beta_T, (0, 1), value=S.zero)\n","    for t in range(T, 0, -1):\n","        beta_stay[t-1] = S.mul(b[:, :-1], stay_scores[t - 1])\n","        beta_move[t-1] = S.mul(b[:, 1:], move_scores[t - 1, :, 1:])\n","        b[:, :-1] = S.sum(torch.stack([beta_stay[t-1], beta_move[t-1]]), dim=0)\n","\n","def dot(x, y, S=Log, dim=-1):\n","    return S.sum(S.mul(x, y), dim=dim) \n","\n","class LogZ(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, stay_scores, move_scores, target_lengths, fwd_bwd_impl):\n","        S = Log\n","        T, N, L = stay_scores.shape\n","        \n","        alpha = stay_scores.new_full((T + 1, N, L), S.zero) \n","        alpha[0, :, 0] = S.one \n","    \n","        beta_stay = stay_scores.new_full((T, N, L), S.zero)\n","        beta_move = stay_scores.new_full((T, N, L), S.zero)\n","        beta_T = stay_scores.new_full((N, L), S.zero) \n","        beta_T[torch.arange(N), target_lengths - 1] = S.one\n","        \n","        fwd_bwd_impl(alpha, beta_T, beta_stay, beta_move, stay_scores, move_scores, S) \n","        \n","        g = torch.softmax(torch.cat([S.mul(alpha[:-1], beta_stay), S.mul(alpha[:-1], beta_move)], dim=2), dim=2) #express softmax in terms of S?\n","        \n","        ctx.save_for_backward(g.reshape(T, N, 2, L))\n","        return dot(alpha[-1], beta_T, S)\n","\n","    @staticmethod\n","    def backward(ctx, grad):\n","        g = ctx.saved_tensors[0] * grad[None, :, None, None]\n","        return g[:, :, 0], g[:, :, 1, :-1], None, None\n","\n","def logZ_py(stay_scores, move_scores, target_lengths):\n","    return LogZ.apply(stay_scores, move_scores, target_lengths, _simple_lattice_fwd_bwd)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pAjA3_--cJeh","colab_type":"code","colab":{}},"source":["#export\n","mean = lambda f: (lambda *xs: f(*xs).mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JG5eP5EtkKeW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1594985764587,"user_tz":-60,"elapsed":11523,"user":{"displayName":"david page","photoUrl":"","userId":"15385526310632231424"}},"outputId":"dd406a4a-daf3-4429-9ea5-e7ac733aa140"},"source":["fwds, bwds = compare_fwd_bwd(float64(mean(logZ_fwd)), float64(mean(logZ_py)), *sample_inputs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fwd diff: 0.00e+00\n","bwd diff: 5.82e-11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a7ZLRSjCoG6i","colab_type":"text"},"source":["### 3. Cupy"]},{"cell_type":"code","metadata":{"id":"fzyYeFSfFPWU","colab_type":"code","colab":{}},"source":["%%writefile cuda/ctc_simple.cu\n","__device__ __forceinline__ FLOAT max2(FLOAT a, FLOAT a1) {\n","    return a > a1 ? a : a1; \n","}\n","\n","__device__ __forceinline__ FLOAT logsumexp2(FLOAT a, FLOAT a1) {\n","    FLOAT maxa = max2(a, a1); \n","    return maxa + log(exp(a-maxa) + exp(a1-maxa));\n","}\n","\n","__device__ __forceinline__ FLOAT add(FLOAT a, FLOAT b) {return a + b;}\n","__device__ __forceinline__ FLOAT mul(FLOAT a, FLOAT b) {return a * b;}\n","\n","extern \"C\" __global__ void fwd_bwd_logspace(\n","    FLOAT* __restrict__ alpha, FLOAT* __restrict__ beta_T,\n","    FLOAT* __restrict__ beta_stay, FLOAT* __restrict__ beta_move, \n","    const FLOAT* __restrict__ stay_scores, const FLOAT* __restrict__ move_scores,\n","    int T, int N, int L\n",") {\n","    int bx = blockIdx.x, tx = threadIdx.x;\n","    if (tx >= L) return;\n","    extern __shared__ FLOAT smem[];\n","    if (blockIdx.y == 0) {\n","        FLOAT a = ZERO, a1 = ZERO;\n","        a = alpha[bx * L + tx];\n","        for (int t = 0; t < T; t++) {\n","            FLOAT *buf = smem + (t % 2) * blockDim.x;\n","            buf[tx] = a; __syncthreads(); \n","            if (tx > 0) {a1 = MUL(move_scores[(t * N + bx) * (L - 1) + tx - 1], buf[tx - 1]);}\n","            a = SUM(MUL(stay_scores[(t * N + bx) * L + tx], a), a1);\n","            alpha[((t + 1) * N + bx) * L + tx] = a;\n","        }\n","    }\n","    else {\n","        FLOAT b = ZERO, b1 = ZERO;\n","        b = beta_T[bx * L + tx];\n","        for (int t = T; t > 0; t--) {\n","            FLOAT *buf = smem + (t % 2) * blockDim.x;\n","            buf[tx] = b; __syncthreads();\n","            if (tx < L - 1) {\n","                b1 = MUL(buf[tx+1], move_scores[(((t - 1) * N + bx) * (L - 1)) + tx]);\n","                beta_move[((t - 1) * N + bx) * (L - 1) + tx] = b1;\n","            }\n","            b = beta_stay[((t - 1) * N + bx) * L + tx] = MUL(b, stay_scores[(((t - 1) * N + bx) * L) + tx]);\n","            b = SUM(b, b1);\n","        }\n","    }\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02YPMAOyMFhi","colab_type":"code","colab":{}},"source":["#export\n","from mctc.utils import *\n","import cupy as cp\n","\n","cupy_funcs = {\n","    (torch.float32, Log): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace', FLOAT='float',  SUM='logsumexp2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n","    (torch.float64, Log): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace', FLOAT='double', SUM='logsumexp2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n","}\n","\n","def _simple_lattice_fwd_bwd_cupy(alpha, beta_T, beta_stay, beta_move, stay_scores, move_scores, S:semiring):\n","    T, N, L = stay_scores.shape\n","    with cp.cuda.Device(stay_scores.device.index):\n","        cupy_funcs[(stay_scores.dtype, S)](grid=(N, 2, 1), block=(L, 1, 1), shared_mem=2*8*L,\n","               args=(alpha.data_ptr(), beta_T.data_ptr(), beta_stay.data_ptr(), beta_move.data_ptr(), \n","                     stay_scores.data_ptr(), move_scores.data_ptr(), T, N, L))\n","\n","def logZ_cupy(stay_scores, move_scores, target_lengths):\n","    return LogZ.apply(stay_scores, move_scores, target_lengths, _simple_lattice_fwd_bwd_cupy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AP6kt-4kdcuP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1594985996490,"user_tz":-60,"elapsed":1632,"user":{"displayName":"david page","photoUrl":"","userId":"15385526310632231424"}},"outputId":"cf86d859-a0f5-442f-dc60-41068c5bab17"},"source":["fwds, bwds = compare_fwd_bwd(float64(mean(logZ_py)), float64(mean(logZ_cupy)), *sample_inputs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fwd diff: 0.00e+00\n","bwd diff: 1.27e-02\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j4Gl3tE8MXvD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1594984146767,"user_tz":-60,"elapsed":3645,"user":{"displayName":"david page","photoUrl":"","userId":"15385526310632231424"}},"outputId":"4a0a3205-574f-47ae-922b-4d79cbfb09c8"},"source":["report(benchmark_fwd_bwd(mean(logZ_cupy), *sample_inputs, nloops=100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fwd: 19.44ms (19.22-19.71ms)\n","bwd: 7.28ms (7.22-7.39ms)\n","tot: 26.72ms (26.45-27.00ms)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"boKaRy7DqCz9","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}