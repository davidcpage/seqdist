{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidcpage/seqdist/blob/master/notebooks/01_CTC_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ctc_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC loss simple\n",
    "\n",
    "> A simplified CTC loss for decoding lattices with only two options stay/move. This can be used for decoding without collapsing of repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from seqdist.utils import *\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a test example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_sample_inputs(T, N, L_min, L_max, device=device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        T: number of time steps\n",
    "        N: batch size\n",
    "        L_min, L_max: bounds on target length\n",
    "    \"\"\"\n",
    "    stay_scores = torch.rand(T, N, L_max, device=device, requires_grad=True)\n",
    "    move_scores = torch.rand(T, N, L_max-1, device=device, requires_grad=True)\n",
    "    target_lengths = torch.randint(L_min, L_max+1, (N,), device=device)\n",
    "    return stay_scores, move_scores, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_inputs = stay_scores, move_scores, target_lengths = generate_sample_inputs(T=800, N=64, L_min=330, L_max=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a straightforward implementation in pytorch in logspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn.functional import pad\n",
    "from seqdist.core import semiring, Log, Max\n",
    "\n",
    "def logZ_fwd(stay_scores, move_scores, target_lengths, S=Log):\n",
    "    T, N, L = stay_scores.shape\n",
    "    alpha_0 = stay_scores.new_full((N, L), S.zero); alpha_0[:, 0] = S.one\n",
    "    beta_T = stay_scores.new_full((N, L), S.zero); beta_T[torch.arange(N), target_lengths - 1] = S.one\n",
    "    move_scores = pad(move_scores, (1, 0), value=S.zero)\n",
    "    a = pad(alpha_0, (1, 0), value=S.zero)\n",
    "    for t in range(0, stay_scores.size(0)):\n",
    "        a[:, 1:] = S.sum(torch.stack([\n",
    "            S.mul(stay_scores[t], a[:, 1:]),\n",
    "            S.mul(move_scores[t], a[:, :-1])\n",
    "        ]), dim=0)    \n",
    "    return S.sum(S.mul(a[:, 1:], beta_T), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([974.2330, 984.3713, 984.7618, 983.5563, 977.9988, 980.5652, 982.5593,\n",
       "        971.0525, 959.5686, 981.7110, 982.6719, 981.7672, 960.4211, 958.2485,\n",
       "        970.2556, 958.7040, 980.2366, 959.4419, 982.2386, 981.1763, 980.1992,\n",
       "        977.2121, 960.9742, 982.7201, 980.7336, 983.1967, 981.9204, 983.6624,\n",
       "        984.1556, 984.8323, 976.8654, 966.7563, 968.9673, 970.4185, 980.5213,\n",
       "        981.7200, 976.7703, 976.3128, 974.6606, 980.0289, 972.0240, 978.2053,\n",
       "        971.4316, 969.5367, 969.0744, 975.5978, 968.7987, 973.7723, 971.9407,\n",
       "        984.4995, 983.2872, 973.0983, 974.6730, 962.9532, 979.3806, 984.2042,\n",
       "        975.2693, 972.7799, 968.5809, 981.6971, 978.7980, 982.7258, 980.3069,\n",
       "        978.9800], device='cuda:0', grad_fn=<LogsumexpBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = logZ_fwd(*sample_inputs)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report(benchmark_fwd_bwd((lambda *x: logZ_fwd(*x).sum()), *sample_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pytorch with grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _simple_lattice_fwd_bwd(alpha, beta_T, beta_stay, beta_move, stay_scores, move_scores, S=Log):\n",
    "    T = alpha.size(0) - 1\n",
    "    move_scores = pad(move_scores, (1, 1), value=S.zero)\n",
    "    a = pad(alpha[0], (1, 0), value=S.zero)\n",
    "    for t in range(0, T):\n",
    "        a[:, 1:] = S.sum(torch.stack([\n",
    "            S.mul(stay_scores[t], a[:, 1:]),\n",
    "            S.mul(move_scores[t, :, :-1], a[:, :-1])\n",
    "        ]), dim=0)\n",
    "        alpha[t+1] = a[:, 1:]\n",
    "    \n",
    "    b = pad(beta_T, (0, 1), value=S.zero)\n",
    "    for t in range(T, 0, -1):\n",
    "        beta_stay[t-1] = S.mul(b[:, :-1], stay_scores[t - 1])\n",
    "        beta_move[t-1] = S.mul(b[:, 1:], move_scores[t - 1, :, 1:])\n",
    "        b[:, :-1] = S.sum(torch.stack([beta_stay[t-1], beta_move[t-1]]), dim=0)\n",
    "\n",
    "def dot(x, y, S=Log, dim=-1):\n",
    "    return S.sum(S.mul(x, y), dim=dim) \n",
    "\n",
    "class LogZ(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, stay_scores, move_scores, target_lengths, fwd_bwd_impl, S:semiring):\n",
    "        T, N, L = stay_scores.shape\n",
    "        \n",
    "        alpha = stay_scores.new_full((T + 1, N, L), S.zero) \n",
    "        alpha[0, :, 0] = S.one \n",
    "    \n",
    "        beta_stay = stay_scores.new_full((T, N, L), S.zero)\n",
    "        beta_move = stay_scores.new_full((T, N, L), S.zero)\n",
    "        beta_T = stay_scores.new_full((N, L), S.zero) \n",
    "        beta_T[torch.arange(N), target_lengths - 1] = S.one\n",
    "        \n",
    "        fwd_bwd_impl(alpha, beta_T, beta_stay, beta_move, stay_scores, move_scores, S) \n",
    "        \n",
    "        g = S.dsum(torch.cat([S.mul(alpha[:-1], beta_stay), S.mul(alpha[:-1], beta_move)], dim=2), dim=2)\n",
    "        \n",
    "        ctx.save_for_backward(g.reshape(T, N, 2, L))\n",
    "        return dot(alpha[-1], beta_T, S)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        g = ctx.saved_tensors[0] * grad[None, :, None, None]\n",
    "        return g[:, :, 0], g[:, :, 1, :-1], None, None, None\n",
    "\n",
    "def logZ_py(stay_scores, move_scores, target_lengths):\n",
    "    return LogZ.apply(stay_scores, move_scores, target_lengths, _simple_lattice_fwd_bwd, Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mean = lambda f: (lambda *xs: f(*xs).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwd diff: 0.00e+00\n",
      "bwd diff: 1.16e-10\n"
     ]
    }
   ],
   "source": [
    "fwds, bwds = compare_fwd_bwd(float64(mean(logZ_fwd)), float64(mean(logZ_py)), *sample_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cupy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: we defined beta_move to have size (T, N, L) not the more natural (T, N, L - 1) above. We did this so that we can stack it with beta_stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile seqdist/cuda/ctc_simple.cu\n",
    "__device__ __forceinline__ FLOAT max2(FLOAT a, FLOAT a1) {\n",
    "    return a > a1 ? a : a1; \n",
    "}\n",
    "\n",
    "__device__ __forceinline__ FLOAT logsumexp2(FLOAT a, FLOAT a1) {\n",
    "    FLOAT maxa = max2(a, a1); \n",
    "    return maxa + log(exp(a-maxa) + exp(a1-maxa));\n",
    "}\n",
    "\n",
    "__device__ __forceinline__ FLOAT add(FLOAT a, FLOAT b) {return a + b;}\n",
    "__device__ __forceinline__ FLOAT mul(FLOAT a, FLOAT b) {return a * b;}\n",
    "\n",
    "extern \"C\" __global__ void fwd_bwd_logspace(\n",
    "    FLOAT* __restrict__ alpha, FLOAT* __restrict__ beta_T,\n",
    "    FLOAT* __restrict__ beta_stay, FLOAT* __restrict__ beta_move, \n",
    "    const FLOAT* __restrict__ stay_scores, const FLOAT* __restrict__ move_scores,\n",
    "    int T, int N, int L\n",
    ") {\n",
    "    int bx = blockIdx.x, tx = threadIdx.x;\n",
    "    if (tx >= L) return;\n",
    "    extern __shared__ FLOAT smem[];\n",
    "    if (blockIdx.y == 0) {\n",
    "        FLOAT a = ZERO, a1 = ZERO;\n",
    "        a = alpha[bx * L + tx];\n",
    "        for (int t = 0; t < T; t++) {\n",
    "            FLOAT *buf = smem + (t % 2) * blockDim.x;\n",
    "            buf[tx] = a; __syncthreads(); \n",
    "            if (tx > 0) {a1 = MUL(move_scores[(t * N + bx) * (L - 1) + tx - 1], buf[tx - 1]);}\n",
    "            a = SUM(MUL(stay_scores[(t * N + bx) * L + tx], a), a1);\n",
    "            alpha[((t + 1) * N + bx) * L + tx] = a;\n",
    "        }\n",
    "    }\n",
    "    else {\n",
    "        FLOAT b = ZERO, b1 = ZERO;\n",
    "        b = beta_T[bx * L + tx];\n",
    "        for (int t = T; t > 0; t--) {\n",
    "            FLOAT *buf = smem + (t % 2) * blockDim.x;\n",
    "            buf[tx] = b; __syncthreads();\n",
    "            if (tx < L - 1) {\n",
    "                b1 = MUL(buf[tx + 1], move_scores[(((t - 1) * N + bx) * (L - 1)) + tx]);\n",
    "                beta_move[((t - 1) * N + bx) * L + tx] = b1;\n",
    "            }\n",
    "            b = MUL(b, stay_scores[(((t - 1) * N + bx) * L) + tx]);\n",
    "            beta_stay[((t - 1) * N + bx) * L + tx] = b;\n",
    "            b = SUM(b, b1);\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "\n",
    "extern \"C\" __global__ void fwd_bwd_logspace_loop(\n",
    "    FLOAT* __restrict__ alpha, FLOAT* __restrict__ beta,\n",
    "    FLOAT* __restrict__ beta_stay, FLOAT* __restrict__ beta_move, \n",
    "    const FLOAT* __restrict__ stay_scores, const FLOAT* __restrict__ move_scores,\n",
    "    int T, int N, int L\n",
    ") {\n",
    "    int bx = blockIdx.x, tx = threadIdx.x;\n",
    "    if (blockIdx.y == 0) {\n",
    "        FLOAT a;\n",
    "        for (int t = 0; t < T; t++) {\n",
    "            for (int j = tx; j < L; j += blockDim.x) {\n",
    "                a = (j > 0) ? MUL(move_scores[(t * N + bx) * (L - 1) + j - 1], alpha[(t * N + bx) * L + j - 1]) : ZERO;\n",
    "                alpha[((t + 1) * N + bx) * L + j] = SUM(MUL(stay_scores[(t * N + bx) * L + j], alpha[(t * N + bx) * L + j]), a);\n",
    "            }\n",
    "            __syncthreads();\n",
    "        }\n",
    "    }\n",
    "    else {\n",
    "        FLOAT b, b1;\n",
    "        for (int t = T; t > 0; t--) {\n",
    "            for (int j = L - blockDim.x + tx; j >= 0; j -= blockDim.x) {\n",
    "                b1 = ZERO;\n",
    "                if (j < L - 1) {\n",
    "                    b1 = MUL(beta[(t * N + bx) * L + j + 1], move_scores[(((t - 1) * N + bx) * (L - 1)) + j]);\n",
    "                    beta_move[((t - 1) * N + bx) * L + j] = b1;\n",
    "                }\n",
    "                b = MUL(beta[(t * N + bx) * L + j], stay_scores[(((t - 1) * N + bx) * L) + j]);\n",
    "                beta_stay[((t - 1) * N + bx) * L + j] = b;\n",
    "                beta[((t - 1) * N + bx) * L + j] = SUM(b, b1);\n",
    "            }\n",
    "            __syncthreads();\n",
    "        }\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from seqdist.utils import *\n",
    "import cupy as cp\n",
    "\n",
    "cupy_funcs = {\n",
    "    (torch.float32, Log): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace', FLOAT='float',  SUM='logsumexp2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n",
    "    (torch.float64, Log): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace', FLOAT='double', SUM='logsumexp2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n",
    "    (torch.float32, Max): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace', FLOAT='float',  SUM='max2', MUL='add', ZERO='{:E}'.format(Max.zero)),\n",
    "    (torch.float64, Max): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace', FLOAT='double', SUM='max2', MUL='add', ZERO='{:E}'.format(Max.zero)),\n",
    "}\n",
    "\n",
    "cupy_funcs_loop = {\n",
    "    (torch.float32, Log): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace_loop', FLOAT='float',  SUM='logsumexp2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n",
    "    (torch.float64, Log): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace_loop', FLOAT='double', SUM='logsumexp2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n",
    "    (torch.float32, Max): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace_loop', FLOAT='float',  SUM='max2', MUL='add', ZERO='{:E}'.format(Max.zero)),\n",
    "    (torch.float64, Max): load_cupy_func('cuda/ctc_simple.cu', 'fwd_bwd_logspace_loop', FLOAT='double', SUM='max2', MUL='add', ZERO='{:E}'.format(Max.zero)),\n",
    "}\n",
    "\n",
    "def _simple_lattice_fwd_bwd_cupy(alpha, beta_T, beta_stay, beta_move, stay_scores, move_scores, S:semiring):\n",
    "    T, N, L = stay_scores.shape\n",
    "    if L > 1024: #exceeds max threads per block\n",
    "        return _simple_lattice_fwd_bwd_cupy_loop(alpha, beta_T, beta_stay, beta_move, stay_scores, move_scores, S)\n",
    "    _bytes = 8 if (stay_scores.dtype == torch.float64) else 4\n",
    "    with cp.cuda.Device(stay_scores.device.index):\n",
    "        cupy_funcs[(stay_scores.dtype, S)](grid=(N, 2, 1), block=(L, 1, 1), shared_mem=2*_bytes*L,\n",
    "               args=(alpha.data_ptr(), beta_T.data_ptr(), beta_stay.data_ptr(), beta_move.data_ptr(),\n",
    "                     stay_scores.data_ptr(), move_scores.data_ptr(), T, N, L))\n",
    "\n",
    "def _simple_lattice_fwd_bwd_cupy_loop(alpha, beta_T, beta_stay, beta_move, stay_scores, move_scores, S:semiring, max_block_size=1024):\n",
    "    T, N, L = stay_scores.shape\n",
    "    block_size = min(L, max_block_size)\n",
    "    beta = alpha.new_full(alpha.shape, S.zero)\n",
    "    beta[-1] = beta_T\n",
    "    with cp.cuda.Device(stay_scores.device.index):\n",
    "        cupy_funcs_loop[(stay_scores.dtype, S)](grid=(N, 2, 1), block=(block_size, 1, 1),\n",
    "               args=(alpha.data_ptr(), beta.data_ptr(), beta_stay.data_ptr(), beta_move.data_ptr(),\n",
    "                     stay_scores.data_ptr(), move_scores.data_ptr(), T, N, L))\n",
    "        \n",
    "def logZ_cupy(stay_scores, move_scores, target_lengths, S:semiring=Log):\n",
    "    return LogZ.apply(stay_scores, move_scores, target_lengths, _simple_lattice_fwd_bwd_cupy, S)\n",
    "\n",
    "def viterbi_alignments(stay_scores, move_scores, target_lengths):\n",
    "    target_lengths = target_lengths.to(stay_scores.device)\n",
    "    stay_scores, move_scores = stay_scores.detach().requires_grad_(), move_scores.detach().requires_grad_()\n",
    "    logZ_cupy(stay_scores, move_scores, target_lengths, Max).sum().backward()\n",
    "    alignments = stay_scores.grad.clone()\n",
    "    alignments[:, :, :-1] += move_scores.grad\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwd diff: 0.00e+00\n",
      "bwd diff: 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "fwds, bwds = compare_fwd_bwd(float64(mean(logZ_py)), float64(mean(logZ_cupy)), *sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwd: 19.56ms (19.31-21.06ms)\n",
      "bwd: 7.53ms (7.40-8.05ms)\n",
      "tot: 27.10ms (26.81-29.11ms)\n"
     ]
    }
   ],
   "source": [
    "report(benchmark_fwd_bwd(mean(logZ_cupy), *sample_inputs, nloops=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
