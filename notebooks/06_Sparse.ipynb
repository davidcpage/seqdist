{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidcpage/seqdist/blob/master/notebooks/01_CTC_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse\n",
    "\n",
    "> Sparse partition function calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial, lru_cache as cache\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import torch\n",
    "\n",
    "from seqdist.core import semiring, Max, Log \n",
    "from seqdist.utils import *\n",
    "from seqdist.ctc import interleave_blanks, generate_sample_inputs, loss_pytorch, benchmark_fwd_bwd, report, compare_fwd_bwd\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def Mv_scan_py(Ms, idx, v0, S:semiring=Log):\n",
    "    T, N, C, nz = Ms.shape\n",
    "    alpha = Ms.new_full((T+1, N, C), S.zero)\n",
    "    alpha[0] = v0 \n",
    "    for t in range(T):\n",
    "        alpha[t+1] = S.sum(S.mul(Ms[t], alpha[t, :, idx]), dim=2)\n",
    "    return alpha\n",
    "\n",
    "class _LogZ_scan(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Ms, idx, v0, vT, S:semiring, scan):\n",
    "        alpha = scan(Ms, idx, v0, S)\n",
    "        ctx.save_for_backward(alpha, Ms, idx, vT)\n",
    "        ctx.semiring, ctx.scan = S, scan\n",
    "        return S.sum(S.mul(alpha[-1], vT), dim=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        alpha, Ms, idx, vT = ctx.saved_tensors\n",
    "        S, scan = ctx.semiring, ctx.scan\n",
    "        T, N, C, nz = Ms.shape\n",
    "        idx_T = idx.flatten().argsort().reshape(*idx.shape) #transpose\n",
    "        Ms_T = Ms.reshape(T, N, -1)[:, :, idx_T]\n",
    "        beta = scan(Ms_T.flip(0), idx_T // nz, vT, S)\n",
    "        g = S.mul(S.mul(Ms.reshape(T, N, -1), alpha[:-1, :, idx.flatten()]).reshape(T, N, C, nz), beta[:-1, :, :, None].flip(0))\n",
    "        g = S.dsum(g.reshape(T, N, -1), dim=2).reshape(T, N, C, nz)\n",
    "        return grad[None, :, None, None] * g, None, None, None, None, None \n",
    "\n",
    "def logZ_scan_py(Ms, idx, v0, vT, S:semiring):\n",
    "    return _LogZ_scan.apply(Ms, idx, v0, vT, S, Mv_scan_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CTC loss using sparse LogZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: This is only as a test/demo - it is slower than the previous CTC loss implementation and only supports the case where all input_lengths are equal to T (although this could be fixed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "def _ctc_loss(logits, targets, input_lengths, target_lengths, logZ_impl, S:semiring=Log):\n",
    "    zero, one = [logits.new_full((1,), x) for x in (S.zero, S.one)]\n",
    "    scores = logits.log_softmax(2)\n",
    "    states = interleave_blanks(targets, blank_idx=0)\n",
    "    state_scores = torch.gather(scores, 2, states.expand(scores.size(0), -1, -1))\n",
    "    final_states = torch.stack([target_lengths*2-1, target_lengths*2], 1)\n",
    "\n",
    "    T, N, Lp = state_scores.shape\n",
    "    assert torch.all(input_lengths == T)\n",
    "\n",
    "    Ms = torch.stack([\n",
    "        state_scores, \n",
    "        pad(state_scores[:, :, 1:], (1, 0), value=S.zero),\n",
    "        pad(torch.where(states[:, 2:] == states[:, :-2], zero.expand(T, N, Lp-2), state_scores[:, :, 2:]), (2, 0), value=S.zero)\n",
    "    ], -1)\n",
    "\n",
    "    i = torch.arange(Lp, device=device)\n",
    "    rot = lambda x, n: torch.cat([x[-n:], x[:-n]])\n",
    "    idx = torch.stack([i, rot(i, 1), rot(i, 2)], dim=1)\n",
    "\n",
    "    v0 = torch.cat([one.expand(N, 1), zero.expand(N, Lp - 1)], dim=1)\n",
    "    vT = zero.expand(N, Lp).clone().scatter_(1, final_states, S.one)\n",
    "    \n",
    "    logZ = logZ_impl(Ms, idx, v0, vT, S)\n",
    "    return -(logZ / target_lengths).mean()\n",
    "\n",
    "ctc_loss_scan_py = partial(_ctc_loss, logZ_impl=logZ_scan_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwd diff: 0.00e+00\n",
      "bwd diff: 8.51e-08\n"
     ]
    }
   ],
   "source": [
    "sample_inputs = logits, targets, input_lengths, target_lengths = generate_sample_inputs(T_min=500, T_max=500, N=128, C=20, L_min=80, L_max=100)\n",
    "fwd, bwd = compare_fwd_bwd(loss_pytorch, ctc_loss_scan_py, *sample_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cuda/sparse_scan.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile cuda/sparse_scan.cu\n",
    "__device__ __forceinline__ FLOAT max2(FLOAT a, FLOAT b) {return a > b ? a : b;}\n",
    "__device__ __forceinline__ FLOAT logsumexp2(FLOAT a, FLOAT b) {return a > b ? log1p(exp(b - a)) + a : log1p(exp(a - b)) + b;}\n",
    "__device__ __forceinline__ FLOAT add(FLOAT a, FLOAT b) {return a + b;}\n",
    "\n",
    "extern \"C\" __global__ void sparse_Mv_scan(\n",
    "    FLOAT* __restrict__ alpha,\n",
    "    const FLOAT* __restrict__ Ms,  \n",
    "    const int* __restrict__ idx,\n",
    "    int T, int N, int C, int nz\n",
    ") {\n",
    "    int bx = blockIdx.x, tx = threadIdx.x;\n",
    "    if (tx >= C) return;\n",
    "    extern __shared__ FLOAT smem[];\n",
    "    \n",
    "    FLOAT a = alpha[bx * C + tx];\n",
    "    for (int t = 0; t < T; t++) {\n",
    "        FLOAT *buf = smem + (t % 2) * blockDim.x;\n",
    "        buf[tx] = a; __syncthreads();      \n",
    "        int i = ((t * N + bx) * C) + tx;\n",
    "        a = MUL(buf[idx[tx * nz]], Ms[i * nz]);\n",
    "        for (int j = 1; j < nz; j++) {\n",
    "            a = ADD(a, MUL(buf[idx[tx * nz + j]], Ms[i * nz + j]));\n",
    "        }\n",
    "        alpha[i + N * C] = a;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "cupy_funcs = {\n",
    "    (torch.float32, Log): load_cupy_func('cuda/sparse_scan.cu', 'sparse_Mv_scan', FLOAT='float',  ADD='logsumexp2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n",
    "    (torch.float64, Log): load_cupy_func('cuda/sparse_scan.cu', 'sparse_Mv_scan', FLOAT='double',  ADD='logsumexp2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n",
    "    (torch.float32, Max): load_cupy_func('cuda/sparse_scan.cu', 'sparse_Mv_scan', FLOAT='float',  ADD='max2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n",
    "    (torch.float64, Max): load_cupy_func('cuda/sparse_scan.cu', 'sparse_Mv_scan', FLOAT='double',  ADD='max2', MUL='add', ZERO='{:E}'.format(Log.zero)),\n",
    "}\n",
    "\n",
    "def Mv_scan_cupy(Ms, idx, v0, S:semiring):\n",
    "    T, N, C, nz = Ms.shape\n",
    "    assert idx.shape == (C, nz) \n",
    "    alpha = Ms.new_full((T+1, N, C), S.zero)\n",
    "    alpha[0] = v0\n",
    "    with cp.cuda.Device(Ms.device.index):\n",
    "        cupy_funcs[(Ms.dtype, S)](grid=(N, 1, 1), block=(C, 1, 1), shared_mem=2*8*C,\n",
    "               args=(alpha.data_ptr(), Ms.data_ptr(), idx.to(dtype=torch.int, device=Ms.device).data_ptr(), T, N, C, nz))\n",
    "    return alpha\n",
    "\n",
    "def logZ_scan(Ms, idx, v0, vT, S:semiring):\n",
    "    return _LogZ_scan.apply(Ms, idx, v0, vT, S, Mv_scan_cupy)\n",
    "\n",
    "ctc_loss_scan = partial(_ctc_loss, logZ_impl=logZ_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwd diff: 0.00e+00\n",
      "bwd diff: 9.95e-08\n"
     ]
    }
   ],
   "source": [
    "sample_inputs = logits, targets, input_lengths, target_lengths = generate_sample_inputs(T_min=500, T_max=500, N=128, C=20, L_min=80, L_max=100)\n",
    "fwd, bwd = compare_fwd_bwd(loss_pytorch, ctc_loss_scan, *sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd: 2.14ms (2.00-2.23ms)\n",
      "fwd: 1.26ms (0.94-6.39ms)\n",
      "tot: 3.40ms (3.00-8.54ms)\n"
     ]
    }
   ],
   "source": [
    "report(benchmark_fwd_bwd(loss_pytorch, *sample_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd: 7.23ms (7.15-7.32ms)\n",
      "fwd: 3.46ms (3.30-4.24ms)\n",
      "tot: 10.69ms (10.50-11.55ms)\n"
     ]
    }
   ],
   "source": [
    "report(benchmark_fwd_bwd(ctc_loss_scan, *sample_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Faster grads in Cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cuda/sparse_logZ.cu\n",
    "__device__ __forceinline__ FLOAT add(FLOAT a, FLOAT b) {return a + b;}\n",
    "__device__ __forceinline__ FLOAT max_(FLOAT *s) {\n",
    "    FLOAT mx = s[0];\n",
    "    for (int j = 1; j < NZ; j++) {\n",
    "        mx = mx > s[j] ? mx : s[j];\n",
    "    }\n",
    "    return mx;\n",
    "}\n",
    "__device__ __forceinline__ FLOAT logsumexp(FLOAT *s) {\n",
    "    FLOAT mx = max_(s);\n",
    "    FLOAT res = exp(s[0] - mx);\n",
    "    for (int j = 1; j < NZ; j++) {\n",
    "        res += exp(s[j] - mx);\n",
    "    }\n",
    "    return log(res) + mx;\n",
    "}\n",
    "    \n",
    "extern \"C\" __global__ void logZ_fwd_bwd(\n",
    "    FLOAT* __restrict__ logZ,\n",
    "    FLOAT* __restrict__ Ms_grad,\n",
    "    const FLOAT* __restrict__ Ms,\n",
    "    const FLOAT* __restrict__ v0,\n",
    "    const FLOAT* __restrict__ vT,\n",
    "    const int* __restrict__ idx,\n",
    "    const int* __restrict__ idx_T,\n",
    "    int T, int N, int C\n",
    ") {\n",
    "    int bx = blockIdx.x;\n",
    "    int tx = threadIdx.x * K;\n",
    "    if (tx >= C) return;\n",
    "    extern __shared__ FLOAT smem[];\n",
    "    \n",
    "    FLOAT a[K];\n",
    "    for (int k = 0; k < K; k++) {\n",
    "        a[k] = v0[bx * C + tx + k]; \n",
    "    }\n",
    "    __syncthreads();\n",
    "    \n",
    "    FLOAT s[NZ];\n",
    "    for (int t = 0; t < T; t++) {\n",
    "        FLOAT *buf = smem + (t % 2) * blockDim.x * K;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            buf[tx+k] = a[k];\n",
    "        }\n",
    "        __syncthreads();\n",
    "        int i = (t * N + bx) * C * NZ;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            for (int j = 0; j < NZ; j++) {\n",
    "                s[j] = MUL(buf[idx[(tx + k) * NZ + j]], Ms[i + (tx + k) * NZ + j]);\n",
    "                Ms_grad[i + (tx + k) * NZ + j] = s[j];\n",
    "            }\n",
    "            a[k] = SUM(s);        \n",
    "        }\n",
    "    }\n",
    "\n",
    "    for (int k = 0; k < K; k++) {\n",
    "        logZ[bx * C + tx + k] = MUL(a[k], vT[bx * C + tx + k]);\n",
    "        a[k] = vT[bx * C + tx + k];\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    for (int t = T - 1; t >= 0; t--) {\n",
    "        FLOAT *buf = smem + (t % 2) * blockDim.x * K;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            buf[tx+k] = a[k];\n",
    "        }\n",
    "        __syncthreads(); \n",
    "        int i = (t * N + bx) * C * NZ;\n",
    "        for (int k = 0; k < K; k++) {\n",
    "            for (int j = 0; j < NZ; j++) {\n",
    "                int ix = idx_T[(tx + k) * NZ + j];\n",
    "                Ms_grad[i + (tx + k) * NZ + j] = MUL(Ms_grad[i + (tx + k) * NZ + j], a[k]);\n",
    "                s[j] = MUL(buf[ix / NZ], Ms[i + ix]);\n",
    "            }            \n",
    "            a[k] = SUM(s);\n",
    "        }        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@cache(None)\n",
    "def cupy_func(dtype, S, NZ, K):\n",
    "    float_types = {torch.float32: 'float', torch.float64: 'double'}\n",
    "    ops = {\n",
    "        Log: {'sum': 'logsumexp', 'mul': 'add'},\n",
    "        Max: {'sum': 'max_', 'mul': 'add'},\n",
    "    }\n",
    "    fname = 'cuda/sparse_logZ.cu'\n",
    "    return load_cupy_func(fname, 'logZ_fwd_bwd', FLOAT=float_types[dtype],  MUL=ops[S]['mul'], ZERO='{:E}'.format(S.zero), SUM=ops[S]['sum'], NZ=NZ, K=K)\n",
    "\n",
    "def _logZ_fwd_bwd_cupy(Ms, idx, v0, vT, S:semiring=Log, K=4):\n",
    "    assert Ms.device.index is not None\n",
    "    T, N, C, NZ = Ms.shape\n",
    "    assert idx.shape == (C, NZ)\n",
    "    idx = idx.to(dtype=torch.int, device=Ms.device)\n",
    "    Ms_grad = Ms.new_full((T, N, C, NZ), S.zero)\n",
    "    logZ = Ms.new_full((N, C), S.zero)\n",
    "    idx_T = idx.flatten().argsort().to(torch.int) #transpose\n",
    "    _bytes = 8 if (Ms.dtype == torch.float64) else 4\n",
    "    with cp.cuda.Device(Ms.device.index):\n",
    "        cupy_func(Ms.dtype, S, NZ, K)(grid=(N, 1, 1), block=(C//K, 1, 1), shared_mem=2*_bytes*C,\n",
    "               args=(logZ.data_ptr(), Ms_grad.data_ptr(), Ms.data_ptr(), v0.data_ptr(), vT.data_ptr(), idx.data_ptr(), idx_T.data_ptr(), T, N, C))\n",
    "    return S.sum(logZ, dim=1), Ms_grad\n",
    "\n",
    "class _LogZ(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Ms, idx, v0, vT, S:semiring, K):\n",
    "        logZ, Ms_grad = _logZ_fwd_bwd_cupy(Ms, idx, v0, vT, S, K)\n",
    "        ctx.save_for_backward(Ms_grad)\n",
    "        ctx.semiring = S\n",
    "        return logZ\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad):\n",
    "        Ms_grad, = ctx.saved_tensors\n",
    "        T, N, C, nz = Ms_grad.shape\n",
    "        Ms_grad = ctx.semiring.dsum(Ms_grad.reshape(T, N, -1), dim=2).reshape(T, N, C, nz)\n",
    "        return grad[None, :, None, None] * Ms_grad, None, None, None, None, None\n",
    "\n",
    "def logZ(Ms, idx, v0, vT, S:semiring=Log, K=1):\n",
    "    return _LogZ.apply(Ms, idx, v0, vT, S, K)\n",
    "\n",
    "ctc_loss = partial(_ctc_loss, logZ_impl=logZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwd diff: 0.00e+00\n",
      "bwd diff: 9.08e-08\n"
     ]
    }
   ],
   "source": [
    "sample_inputs = logits, targets, input_lengths, target_lengths = generate_sample_inputs(T_min=500, T_max=500, N=128, C=20, L_min=80, L_max=100)\n",
    "fwd, bwd = compare_fwd_bwd(loss_pytorch, ctc_loss, *sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd: 2.17ms (2.13-2.23ms)\n",
      "fwd: 1.07ms (1.03-1.10ms)\n",
      "tot: 3.24ms (3.17-3.32ms)\n"
     ]
    }
   ],
   "source": [
    "report(benchmark_fwd_bwd(loss_pytorch, *sample_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd: 4.03ms (3.99-4.07ms)\n",
      "fwd: 4.29ms (4.23-4.34ms)\n",
      "tot: 8.32ms (8.26-8.37ms)\n"
     ]
    }
   ],
   "source": [
    "report(benchmark_fwd_bwd(ctc_loss, *sample_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
