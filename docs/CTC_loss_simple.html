---

title: CTC loss simple


keywords: fastai
sidebar: home_sidebar

summary: "A simplified CTC loss for decoding lattices with only two options stay/move. This can be used for decoding without collapsing of repeats."
description: "A simplified CTC loss for decoding lattices with only two options stay/move. This can be used for decoding without collapsing of repeats."
nb_path: "notebooks/03_CTC_loss_simple.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/03_CTC_loss_simple.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://colab.research.google.com/github/davidcpage/mctc/blob/master/notebooks/01_CTC_loss.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preliminaries">Preliminaries<a class="anchor-link" href="#Preliminaries"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Generate a test example:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="generate_sample_inputs" class="doc_header"><code>generate_sample_inputs</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc_simple.py#L16" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>generate_sample_inputs</code>(<strong><code>T</code></strong>, <strong><code>N</code></strong>, <strong><code>L_min</code></strong>, <strong><code>L_max</code></strong>, <strong><code>device</code></strong>=<em><code>device(type='cuda')</code></em>)</p>
</blockquote>
<p>Args:
    T: number of time steps
    N: batch size
    L_min, L_max: bounds on target length</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_inputs</span> <span class="o">=</span> <span class="n">stay_scores</span><span class="p">,</span> <span class="n">move_scores</span><span class="p">,</span> <span class="n">target_lengths</span> <span class="o">=</span> <span class="n">generate_sample_inputs</span><span class="p">(</span><span class="n">T</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">L_min</span><span class="o">=</span><span class="mi">330</span><span class="p">,</span> <span class="n">L_max</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-implementations">Loss implementations<a class="anchor-link" href="#Loss-implementations"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-Basic-pytorch">1. Basic pytorch<a class="anchor-link" href="#1.-Basic-pytorch"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here's a straightforward implementation in pytorch in logspace.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="logZ_fwd" class="doc_header"><code>logZ_fwd</code><a href="__main__.py#L5" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>logZ_fwd</code>(<strong><code>stay_scores</code></strong>, <strong><code>move_scores</code></strong>, <strong><code>target_lengths</code></strong>, <strong><code>S</code></strong>=<em><code>semiring(zero=-1e+38, one=0.0, mul=&lt;built-in method add of type object at 0x7f26bed62a80&gt;, sum=&lt;built-in method logsumexp of type object at 0x7f26bed62a80&gt;, dsum=&lt;built-in method softmax of type object at 0x7f26bed62a80&gt;)</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">logZ_fwd</span><span class="p">(</span><span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span>
<span class="n">res</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([974.2330, 984.3713, 984.7618, 983.5563, 977.9988, 980.5652, 982.5593,
        971.0525, 959.5686, 981.7110, 982.6719, 981.7672, 960.4211, 958.2485,
        970.2556, 958.7040, 980.2366, 959.4419, 982.2386, 981.1763, 980.1992,
        977.2121, 960.9742, 982.7201, 980.7336, 983.1967, 981.9204, 983.6624,
        984.1556, 984.8323, 976.8654, 966.7563, 968.9673, 970.4185, 980.5213,
        981.7200, 976.7703, 976.3128, 974.6606, 980.0289, 972.0240, 978.2053,
        971.4316, 969.5367, 969.0744, 975.5978, 968.7987, 973.7723, 971.9407,
        984.4995, 983.2872, 973.0983, 974.6730, 962.9532, 979.3806, 984.2042,
        975.2693, 972.7799, 968.5809, 981.6971, 978.7980, 982.7258, 980.3069,
        978.9800], device=&#39;cuda:0&#39;, grad_fn=&lt;LogsumexpBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-Pytorch-with-grad">2. Pytorch with grad<a class="anchor-link" href="#2.-Pytorch-with-grad"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="dot" class="doc_header"><code>dot</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc_simple.py#L63" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>dot</code>(<strong><code>x</code></strong>, <strong><code>y</code></strong>, <strong><code>S</code></strong>=<em><code>semiring(zero=-1e+38, one=0.0, mul=&lt;built-in method add of type object at 0x7f26bed62a80&gt;, sum=&lt;built-in method logsumexp of type object at 0x7f26bed62a80&gt;, dsum=&lt;built-in method softmax of type object at 0x7f26bed62a80&gt;)</code></em>, <strong><code>dim</code></strong>=<em><code>-1</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LogZ" class="doc_header"><code>class</code> <code>LogZ</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc_simple.py#L66" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LogZ</code>() :: <code>Function</code></p>
</blockquote>
<p>Records operation history and defines formulas for differentiating ops.</p>
<p>See the Note on extending the autograd engine for more details on how to use
this class: <a href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd</a></p>
<p>Every operation performed on :class:<code>Tensor</code> s creates a new function
object, that performs the computation, and records that it happened.
The history is retained in the form of a DAG of functions, with edges
denoting data dependencies (<code>input &lt;- output</code>). Then, when backward is
called, the graph is processed in the topological ordering, by calling
:func:<code>backward</code> methods of each :class:<code>Function</code> object, and passing
returned gradients on to next :class:<code>Function</code> s.</p>
<p>Normally, the only way users interact with functions is by creating
subclasses and defining new operations. This is a recommended way of
extending torch.autograd.</p>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; class Exp(Function):
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def forward(ctx, i):
&gt;&gt;&gt;         result = i.exp()
&gt;&gt;&gt;         ctx.save_for_backward(result)
&gt;&gt;&gt;         return result
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def backward(ctx, grad_output):
&gt;&gt;&gt;         result, = ctx.saved_tensors
&gt;&gt;&gt;         return grad_output * result
&gt;&gt;&gt;
&gt;&gt;&gt; #Use it by calling the apply method:
&gt;&gt;&gt; output = Exp.apply(input)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="logZ_py" class="doc_header"><code>logZ_py</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc_simple.py#L92" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>logZ_py</code>(<strong><code>stay_scores</code></strong>, <strong><code>move_scores</code></strong>, <strong><code>target_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fwds</span><span class="p">,</span> <span class="n">bwds</span> <span class="o">=</span> <span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">float64</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">logZ_fwd</span><span class="p">)),</span> <span class="n">float64</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">logZ_py</span><span class="p">)),</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 1.16e-10
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Cupy">3. Cupy<a class="anchor-link" href="#3.-Cupy"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>NB: we defined beta_move to have size (T, N, L) not the more natural (T, N, L - 1) above. We did this so that we can stack it with beta_stay.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">writefile</span> seqdist/cuda/ctc_simple.cu
__device__ __forceinline__ FLOAT max2(FLOAT a, FLOAT a1) {
    return a &gt; a1 ? a : a1; 
}

__device__ __forceinline__ FLOAT logsumexp2(FLOAT a, FLOAT a1) {
    FLOAT maxa = max2(a, a1); 
    return maxa + log(exp(a-maxa) + exp(a1-maxa));
}

__device__ __forceinline__ FLOAT add(FLOAT a, FLOAT b) {return a + b;}
__device__ __forceinline__ FLOAT mul(FLOAT a, FLOAT b) {return a * b;}

extern &quot;C&quot; __global__ void fwd_bwd_logspace(
    FLOAT* __restrict__ alpha, FLOAT* __restrict__ beta_T,
    FLOAT* __restrict__ beta_stay, FLOAT* __restrict__ beta_move, 
    const FLOAT* __restrict__ stay_scores, const FLOAT* __restrict__ move_scores,
    int T, int N, int L
) {
    int bx = blockIdx.x, tx = threadIdx.x;
    if (tx &gt;= L) return;
    extern __shared__ FLOAT smem[];
    if (blockIdx.y == 0) {
        FLOAT a = ZERO, a1 = ZERO;
        a = alpha[bx * L + tx];
        for (int t = 0; t &lt; T; t++) {
            FLOAT *buf = smem + (t % 2) * blockDim.x;
            buf[tx] = a; __syncthreads(); 
            if (tx &gt; 0) {a1 = MUL(move_scores[(t * N + bx) * (L - 1) + tx - 1], buf[tx - 1]);}
            a = SUM(MUL(stay_scores[(t * N + bx) * L + tx], a), a1);
            alpha[((t + 1) * N + bx) * L + tx] = a;
        }
    }
    else {
        FLOAT b = ZERO, b1 = ZERO;
        b = beta_T[bx * L + tx];
        for (int t = T; t &gt; 0; t--) {
            FLOAT *buf = smem + (t % 2) * blockDim.x;
            buf[tx] = b; __syncthreads();
            if (tx &lt; L - 1) {
                b1 = MUL(buf[tx + 1], move_scores[(((t - 1) * N + bx) * (L - 1)) + tx]);
                beta_move[((t - 1) * N + bx) * L + tx] = b1;
            }
            b = MUL(b, stay_scores[(((t - 1) * N + bx) * L) + tx]);
            beta_stay[((t - 1) * N + bx) * L + tx] = b;
            b = SUM(b, b1);
        }
    }
  }
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="logZ_cupy" class="doc_header"><code>logZ_cupy</code><a href="__main__.py#L17" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>logZ_cupy</code>(<strong><code>stay_scores</code></strong>, <strong><code>move_scores</code></strong>, <strong><code>target_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fwds</span><span class="p">,</span> <span class="n">bwds</span> <span class="o">=</span> <span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">float64</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">logZ_py</span><span class="p">)),</span> <span class="n">float64</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">logZ_cupy</span><span class="p">)),</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 0.00e+00
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">report</span><span class="p">(</span><span class="n">benchmark_fwd_bwd</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">logZ_cupy</span><span class="p">),</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">,</span> <span class="n">nloops</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd: 19.56ms (19.31-21.06ms)
bwd: 7.53ms (7.40-8.05ms)
tot: 27.10ms (26.81-29.11ms)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

