---

title: Normalisation

keywords: fastai
sidebar: home_sidebar

summary: "Partition function calculations."
description: "Partition function calculations."
nb_path: "notebooks/04_Normalisation.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/04_Normalisation.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://colab.research.google.com/github/davidcpage/mctc/blob/master/notebooks/01_CTC_loss.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preliminaries">Preliminaries<a class="anchor-link" href="#Preliminaries"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Generate a test example:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="generate_test_example" class="doc_header"><code>generate_test_example</code><a href="https://github.com/davidcpage/mctc/tree/master/mctc/normalisation.py#L14" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>generate_test_example</code>(<strong><code>T</code></strong>, <strong><code>N</code></strong>, <strong><code>n_state</code></strong>, <strong><code>dtype</code></strong>=<em><code>torch.float32</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-Basic-pytorch">1. Basic pytorch<a class="anchor-link" href="#1.-Basic-pytorch"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="mi">4032</span><span class="o">//</span><span class="mi">5</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">n_state</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">Ms</span> <span class="o">=</span> <span class="n">generate_test_example</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">n_state</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ScriptFunction object at 0x7f8326ac4090>" class="doc_header"><code>ScriptFunction object at 0x7f8326ac4090></code><a href="" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ScriptFunction object at 0x7f8326ac4090></code>()</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Cupy">3. Cupy<a class="anchor-link" href="#3.-Cupy"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%writefile</span> cuda/fused_bmv.cu
<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">max2</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">a1</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">a1</span> <span class="err">?</span> <span class="n">a</span> <span class="p">:</span> <span class="n">a1</span><span class="p">;</span> 
<span class="p">}</span>

<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">logsumexp2</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">a1</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">FLOAT</span> <span class="n">maxa</span> <span class="o">=</span> <span class="n">max2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a1</span><span class="p">);</span> 
    <span class="k">return</span> <span class="n">maxa</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">maxa</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">a1</span><span class="o">-</span><span class="n">maxa</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">add</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span><span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;}</span>
<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">mul</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span><span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">;}</span>

<span class="n">extern</span> <span class="s2">&quot;C&quot;</span> <span class="n">__global__</span> <span class="n">void</span> <span class="n">fwd</span><span class="p">(</span>
    <span class="n">FLOAT</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">alpha</span><span class="p">,</span>
    <span class="n">const</span> <span class="n">FLOAT</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">Ms</span><span class="p">,</span> 
    <span class="nb">int</span> <span class="n">T</span><span class="p">,</span> <span class="nb">int</span> <span class="n">N</span><span class="p">,</span> <span class="nb">int</span> <span class="n">n_state</span>
<span class="p">)</span> <span class="p">{</span>
    <span class="o">//</span> <span class="n">Ms</span> <span class="ow">is</span> <span class="n">shape</span> <span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">n_state</span><span class="p">,</span> <span class="n">n_state</span><span class="p">)</span>
    <span class="o">//</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="n">shape</span> <span class="p">(</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">n_state</span><span class="p">)</span>
    <span class="o">//</span> <span class="n">assumes</span> <span class="n">blockDim</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">threadDim</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="nb">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tx</span> <span class="o">&gt;=</span> <span class="n">n_state</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
    <span class="n">FLOAT</span> <span class="n">u</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">T</span><span class="p">;</span> <span class="n">t</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="nb">int</span> <span class="n">j</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">bx</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_state</span><span class="p">;</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">MUL</span><span class="p">(</span><span class="n">Ms</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">tx</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_state</span><span class="p">],</span> <span class="n">alpha</span><span class="p">[</span><span class="n">j</span><span class="p">]);</span>
        <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n_state</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">u</span> <span class="o">=</span> <span class="n">SUM</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">MUL</span><span class="p">(</span><span class="n">Ms</span><span class="p">[(</span><span class="n">j</span> <span class="o">+</span> <span class="n">tx</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_state</span> <span class="o">+</span> <span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="n">i</span><span class="p">]));</span>
        <span class="p">}</span>
        <span class="n">alpha</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">n_state</span><span class="p">)</span> <span class="o">+</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span><span class="p">;</span>
        <span class="n">__syncthreads</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="fused_batch_Mv" class="doc_header"><code>fused_batch_Mv</code><a href="__main__.py#L12" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>fused_batch_Mv</code>(<strong><code>Ms</code></strong>, <strong><code>alpha_0</code></strong>, <strong><code>S</code></strong>:<a href="/mctc/CTC_loss.html#semiring"><code>semiring</code></a>=<em><code>semiring(zero=-1e+38, one=0.0, mul=&lt;built-in method add of type object at 0x7f8387f42620&gt;, sum=&lt;built-in method logsumexp of type object at 0x7f8387f42620&gt;, dsum=&lt;built-in method softmax of type object at 0x7f8387f42620&gt;)</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LogZ" class="doc_header"><code>class</code> <code>LogZ</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LogZ</code>() :: <code>Function</code></p>
</blockquote>
<p>Records operation history and defines formulas for differentiating ops.</p>
<p>See the Note on extending the autograd engine for more details on how to use
this class: <a href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd</a></p>
<p>Every operation performed on :class:<code>Tensor</code> s creates a new function
object, that performs the computation, and records that it happened.
The history is retained in the form of a DAG of functions, with edges
denoting data dependencies (<code>input &lt;- output</code>). Then, when backward is
called, the graph is processed in the topological ordering, by calling
:func:<code>backward</code> methods of each :class:<code>Function</code> object, and passing
returned gradients on to next :class:<code>Function</code> s.</p>
<p>Normally, the only way users interact with functions is by creating
subclasses and defining new operations. This is a recommended way of
extending torch.autograd.</p>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; class Exp(Function):
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def forward(ctx, i):
&gt;&gt;&gt;         result = i.exp()
&gt;&gt;&gt;         ctx.save_for_backward(result)
&gt;&gt;&gt;         return result
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def backward(ctx, grad_output):
&gt;&gt;&gt;         result, = ctx.saved_tensors
&gt;&gt;&gt;         return grad_output * result
&gt;&gt;&gt;
&gt;&gt;&gt; #Use it by calling the apply method:
&gt;&gt;&gt; output = Exp.apply(input)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LogZViterbi" class="doc_header"><code>class</code> <code>LogZViterbi</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LogZViterbi</code>() :: <code>Function</code></p>
</blockquote>
<p>Records operation history and defines formulas for differentiating ops.</p>
<p>See the Note on extending the autograd engine for more details on how to use
this class: <a href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd</a></p>
<p>Every operation performed on :class:<code>Tensor</code> s creates a new function
object, that performs the computation, and records that it happened.
The history is retained in the form of a DAG of functions, with edges
denoting data dependencies (<code>input &lt;- output</code>). Then, when backward is
called, the graph is processed in the topological ordering, by calling
:func:<code>backward</code> methods of each :class:<code>Function</code> object, and passing
returned gradients on to next :class:<code>Function</code> s.</p>
<p>Normally, the only way users interact with functions is by creating
subclasses and defining new operations. This is a recommended way of
extending torch.autograd.</p>
<p>Examples::</p>

<pre><code>&gt;&gt;&gt; class Exp(Function):
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def forward(ctx, i):
&gt;&gt;&gt;         result = i.exp()
&gt;&gt;&gt;         ctx.save_for_backward(result)
&gt;&gt;&gt;         return result
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def backward(ctx, grad_output):
&gt;&gt;&gt;         result, = ctx.saved_tensors
&gt;&gt;&gt;         return grad_output * result
&gt;&gt;&gt;
&gt;&gt;&gt; #Use it by calling the apply method:
&gt;&gt;&gt; output = Exp.apply(input)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="logz" class="doc_header"><code>logz</code><a href="__main__.py#L45" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>logz</code>(<strong><code>Ms</code></strong>, <strong><code>alpha_0</code></strong>, <strong><code>beta_T</code></strong>, <strong><code>S</code></strong>:<a href="/mctc/CTC_loss.html#semiring"><code>semiring</code></a>=<em><code>semiring(zero=-1e+38, one=0.0, mul=&lt;built-in method add of type object at 0x7f8387f42620&gt;, sum=&lt;built-in method logsumexp of type object at 0x7f8387f42620&gt;, dsum=&lt;built-in method softmax of type object at 0x7f8387f42620&gt;)</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

