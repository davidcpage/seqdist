---

title: CTC loss

keywords: fastai
sidebar: home_sidebar

summary: "A simple and somewhat efficient implementation of the standard <a href='http://www.cs.toronto.edu/~graves/icml_2006.pdf'>CTC loss function</a> using pytorch and cupy."
description: "A simple and somewhat efficient implementation of the standard <a href='http://www.cs.toronto.edu/~graves/icml_2006.pdf'>CTC loss function</a> using pytorch and cupy."
nb_path: "notebooks/02_CTC_loss.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/02_CTC_loss.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://colab.research.google.com/github/davidcpage/seqdist/blob/master/notebooks/01_CTC_loss.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>nvidia-smi
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fri May 29 09:22:22 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   72C    P8    82W / 149W |      0MiB / 11441MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preliminaries">Preliminaries<a class="anchor-link" href="#Preliminaries"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Generate a test example:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="generate_sample_inputs" class="doc_header"><code>generate_sample_inputs</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L19" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>generate_sample_inputs</code>(<strong><code>T_min</code></strong>, <strong><code>T_max</code></strong>, <strong><code>N</code></strong>, <strong><code>C</code></strong>, <strong><code>L_min</code></strong>, <strong><code>L_max</code></strong>, <strong><code>device</code></strong>=<em><code>device(type='cuda')</code></em>)</p>
</blockquote>
<p>Args:
    T_min, T_max: bounds on number of time steps
    N: batch size
    C: alphabet size (including blank)
    L_min, L_max: bounds on target length</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>NB: The pytorch (1.5.0) CTC loss has inconsistent behaviour for input logits which haven't been through log_softmax. The standard pytorch backend produces an incorrect gradient in this case and the standard and Cudnn backends return different results as demonstrated here:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span> <span class="o">=</span> <span class="n">generate_sample_inputs</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">targets_cudnn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">t</span><span class="p">[:</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="c1">#triggers the cudnn backend </span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;log_softmax&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;torch:  </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;cudnn: </span><span class="si">{:.4f}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">targets_cudnn</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;no log_softmax&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;torch:  </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;cudnn:  </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">targets_cudnn</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>log_softmax
torch:  6.0089
cudnn:  6.0089

no log_softmax
torch:  -2.8939
cudnn:  6.0089
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To avoid the issue, let's define all our CTC losses to accept logits and apply log_softmax internally (similarly to what the Cudnn loss does):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="loss_pytorch" class="doc_header"><code>loss_pytorch</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L34" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>loss_pytorch</code>(<strong><code>logits</code></strong>, <strong><code>targets</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>target_lengths</code></strong>, <strong><code>blank</code></strong>=<em><code>0</code></em>, <strong><code>reduction</code></strong>=<em><code>'mean'</code></em>, <strong><code>zero_infinity</code></strong>=<em><code>False</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_inputs</span> <span class="o">=</span> <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span> <span class="o">=</span> <span class="n">generate_sample_inputs</span><span class="p">(</span><span class="n">T_min</span><span class="o">=</span><span class="mi">450</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">L_min</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">L_max</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">loss_pytorch</span><span class="p">(</span><span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>12.637479782104492</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">report</span><span class="p">(</span><span class="n">benchmark_fwd_bwd</span><span class="p">(</span><span class="n">loss_pytorch</span><span class="p">,</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd: 4.70ms (3.96-5.30ms)
bwd: 9.80ms (8.59-10.84ms)
tot: 14.50ms (13.15-15.57ms)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loss-implementations">Loss implementations<a class="anchor-link" href="#Loss-implementations"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1.-Basic-pytorch">1. Basic pytorch<a class="anchor-link" href="#1.-Basic-pytorch"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here's a straightforward implementation in pytorch. As is standard, we're working in logspace for numerical stability. In the interests of clarity, we've encapsulated the logspace arithmetic into an object: Log.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="interleave_blanks" class="doc_header"><code>interleave_blanks</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L39" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>interleave_blanks</code>(<strong><code>targets</code></strong>, <strong><code>blank_idx</code></strong>:<code>int</code>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="prepare_inputs" class="doc_header"><code>prepare_inputs</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L45" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>prepare_inputs</code>(<strong><code>scores</code></strong>, <strong><code>targets</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>target_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="loss_basic" class="doc_header"><code>loss_basic</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L63" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>loss_basic</code>(<strong><code>logits</code></strong>, <strong><code>targets</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>target_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It works:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">float64</span><span class="p">(</span><span class="n">loss_pytorch</span><span class="p">),</span> <span class="n">float64</span><span class="p">(</span><span class="n">loss_basic</span><span class="p">),</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 4.55e-13
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">loss_pytorch</span><span class="p">,</span> <span class="n">loss_basic</span><span class="p">,</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 1.10e-07
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>But it's very slow (roughly 100x slower than the native pytorch version):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">report</span><span class="p">(</span><span class="n">benchmark_fwd_bwd</span><span class="p">(</span><span class="n">loss_basic</span><span class="p">,</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd: 212.33ms (196.86-240.76ms)
bwd: 1057.07ms (1056.57-1057.61ms)
tot: 1269.41ms (1253.75-1297.93ms)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2.-Pytorch-with-grad">2. Pytorch with grad<a class="anchor-link" href="#2.-Pytorch-with-grad"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our aim here is twofold. First we want to compute grads explicitly rather than relying on pytorch autograd. It's a good idea to do this in pytorch to iron out bugs before dropping to cuda.</p>
<p>Secondly we want to restructure the code ready to call a cuda implementation of the inner loop. This requires an outer function (fwd_bwd) to setup result arrays which gets filled in by the inner loop (_fwd_bwd_py or later on _fwd_bwd_cupy.)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="fwd_bwd" class="doc_header"><code>fwd_bwd</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L69" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>fwd_bwd</code>(<strong><code>state_scores</code></strong>, <strong><code>repeat_mask</code></strong>, <strong><code>final_states</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>fwd_bwd_impl</code></strong>, <strong><code>S</code></strong>:<a href="/seqdist/core.html#semiring"><code>semiring</code></a>=<em><code>semiring(zero=-1e+38, one=0.0, mul=&lt;built-in method add of type object at 0x7fc3e6539320&gt;, sum=&lt;built-in method logsumexp of type object at 0x7fc3e6539320&gt;, dsum=&lt;built-in method softmax of type object at 0x7fc3e6539320&gt;)</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="masked_grad" class="doc_header"><code>masked_grad</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L96" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>masked_grad</code>(<strong><code>grad</code></strong>, <strong><code>input_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="loss_py" class="doc_header"><code>loss_py</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L114" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>loss_py</code>(<strong><code>logits</code></strong>, <strong><code>targets</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>target_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fwds</span><span class="p">,</span> <span class="n">bwds</span> <span class="o">=</span> <span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">float64</span><span class="p">(</span><span class="n">loss_pytorch</span><span class="p">),</span> <span class="n">float64</span><span class="p">(</span><span class="n">loss_py</span><span class="p">,</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 4.55e-13
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fwds</span><span class="p">,</span> <span class="n">bwds</span> <span class="o">=</span> <span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">loss_pytorch</span><span class="p">,</span> <span class="n">loss_py</span><span class="p">,</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 1.00e-07
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">report</span><span class="p">(</span><span class="n">benchmark_fwd_bwd</span><span class="p">(</span><span class="n">loss_py</span><span class="p">,</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd: 534.67ms (504.20-596.38ms)
bwd: 6.62ms (6.58-6.78ms)
tot: 541.29ms (510.89-602.96ms)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.-Cupy">3. Cupy<a class="anchor-link" href="#3.-Cupy"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%writefile</span> cuda/ctc.cu
<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">max3</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">a1</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">a2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">FLOAT</span> <span class="n">maxa</span> <span class="o">=</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">a1</span> <span class="err">?</span> <span class="n">a</span> <span class="p">:</span> <span class="n">a1</span><span class="p">;</span> 
    <span class="k">return</span> <span class="n">maxa</span> <span class="o">&gt;</span> <span class="n">a2</span> <span class="err">?</span> <span class="n">maxa</span> <span class="p">:</span> <span class="n">a2</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">logsumexp3</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">a1</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">a2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">FLOAT</span> <span class="n">maxa</span> <span class="o">=</span> <span class="n">max3</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">);</span> 
    <span class="k">return</span> <span class="n">maxa</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">maxa</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">a1</span><span class="o">-</span><span class="n">maxa</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="n">a2</span><span class="o">-</span><span class="n">maxa</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">sum3</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">a1</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">a2</span><span class="p">)</span> <span class="p">{</span><span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">a1</span> <span class="o">+</span> <span class="n">a2</span><span class="p">;}</span>
<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">add</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span><span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;}</span>
<span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">FLOAT</span> <span class="n">mul</span><span class="p">(</span><span class="n">FLOAT</span> <span class="n">a</span><span class="p">,</span> <span class="n">FLOAT</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span><span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">;}</span>

<span class="n">extern</span> <span class="s2">&quot;C&quot;</span> <span class="n">__global__</span> <span class="n">void</span> <span class="n">fwd_bwd_logspace</span><span class="p">(</span>
    <span class="n">FLOAT</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">alpha_T</span><span class="p">,</span> 
    <span class="n">FLOAT</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">FLOAT</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">beta</span><span class="p">,</span>  
    <span class="n">const</span> <span class="n">FLOAT</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">scores</span><span class="p">,</span>  <span class="n">const</span> <span class="nb">bool</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">repeat_mask</span><span class="p">,</span> 
    <span class="n">const</span> <span class="n">long</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">lengths</span><span class="p">,</span>
    <span class="nb">int</span> <span class="n">N</span><span class="p">,</span> <span class="nb">int</span> <span class="n">L</span>
<span class="p">)</span> <span class="p">{</span>
    <span class="nb">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tx</span> <span class="o">&gt;=</span> <span class="n">L</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
    <span class="n">extern</span> <span class="n">__shared__</span> <span class="n">FLOAT</span> <span class="n">smem</span><span class="p">[];</span>
    <span class="nb">int</span> <span class="n">T</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="n">lengths</span><span class="p">[</span><span class="n">bx</span><span class="p">];</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">FLOAT</span> <span class="n">a</span> <span class="o">=</span> <span class="n">ZERO</span><span class="p">,</span> <span class="n">a1</span> <span class="o">=</span> <span class="n">ZERO</span><span class="p">,</span> <span class="n">a2</span> <span class="o">=</span> <span class="n">ZERO</span><span class="p">;</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">[</span><span class="n">bx</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">T</span><span class="p">;</span> <span class="n">t</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">FLOAT</span> <span class="o">*</span><span class="n">buf</span> <span class="o">=</span> <span class="n">smem</span> <span class="o">+</span> <span class="p">(</span><span class="n">t</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">blockDim</span><span class="o">.</span><span class="n">x</span><span class="p">;</span>
            <span class="n">buf</span><span class="p">[</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">;</span> <span class="n">__syncthreads</span><span class="p">();</span> 
            <span class="k">if</span> <span class="p">(</span><span class="n">tx</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="n">a1</span> <span class="o">=</span> <span class="n">buf</span><span class="p">[</span><span class="n">tx</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">tx</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="err">!</span><span class="n">repeat_mask</span><span class="p">[</span><span class="n">bx</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">tx</span><span class="p">])</span> <span class="n">a2</span> <span class="o">=</span> <span class="n">buf</span><span class="p">[</span><span class="n">tx</span> <span class="o">-</span> <span class="mi">2</span><span class="p">];</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">MUL</span><span class="p">(</span><span class="n">scores</span><span class="p">[(</span><span class="n">t</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">bx</span><span class="p">)</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">tx</span><span class="p">],</span> <span class="n">SUM</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">));</span>
            <span class="n">alpha</span><span class="p">[((</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">bx</span><span class="p">)</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">alpha_T</span><span class="p">[</span><span class="n">bx</span><span class="o">*</span><span class="n">L</span> <span class="o">+</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">else</span> <span class="p">{</span>
        <span class="n">FLOAT</span> <span class="n">b</span> <span class="o">=</span> <span class="n">ZERO</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">ZERO</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">ZERO</span><span class="p">;</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[(</span><span class="n">T</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">bx</span><span class="p">)</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="nb">int</span> <span class="n">t</span> <span class="o">=</span> <span class="n">T</span><span class="p">;</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">FLOAT</span> <span class="o">*</span><span class="n">buf</span> <span class="o">=</span> <span class="n">smem</span> <span class="o">+</span> <span class="p">(</span><span class="n">t</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">blockDim</span><span class="o">.</span><span class="n">x</span><span class="p">;</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">buf</span><span class="p">[</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">MUL</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">scores</span><span class="p">[(((</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">bx</span><span class="p">)</span> <span class="o">*</span> <span class="n">L</span><span class="p">)</span> <span class="o">+</span> <span class="n">tx</span><span class="p">]);</span>
            <span class="n">__syncthreads</span><span class="p">();</span> 
            <span class="k">if</span> <span class="p">(</span><span class="n">tx</span> <span class="o">&lt;</span> <span class="n">L</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">buf</span><span class="p">[</span><span class="n">tx</span><span class="o">+</span><span class="mi">1</span><span class="p">];</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">tx</span> <span class="o">&lt;</span> <span class="n">L</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="err">!</span><span class="n">repeat_mask</span><span class="p">[</span><span class="n">bx</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">tx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">])</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">buf</span><span class="p">[</span><span class="n">tx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[((</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">bx</span><span class="p">)</span> <span class="o">*</span> <span class="n">L</span> <span class="o">+</span> <span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">SUM</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Overwriting ctc.cu
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="loss_cupy" class="doc_header"><code>loss_cupy</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L133" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>loss_cupy</code>(<strong><code>logits</code></strong>, <strong><code>targets</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>target_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fwds</span><span class="p">,</span> <span class="n">bwds</span> <span class="o">=</span> <span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">float64</span><span class="p">(</span><span class="n">loss_pytorch</span><span class="p">),</span> <span class="n">float64</span><span class="p">(</span><span class="n">loss_cupy</span><span class="p">),</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 4.55e-13
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fwds</span><span class="p">,</span> <span class="n">bwds</span> <span class="o">=</span> <span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">loss_pytorch</span><span class="p">,</span> <span class="n">loss_cupy</span><span class="p">,</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 1.00e-07
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">report</span><span class="p">(</span><span class="n">benchmark_fwd_bwd</span><span class="p">(</span><span class="n">loss_cupy</span><span class="p">,</span> <span class="o">*</span><span class="n">sample_inputs</span><span class="p">,</span> <span class="n">nloops</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd: 6.51ms (5.94-8.54ms)
bwd: 5.18ms (4.83-6.53ms)
tot: 11.69ms (10.78-14.93ms)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Viterbi-alignments">Viterbi alignments<a class="anchor-link" href="#Viterbi-alignments"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="viterbi_alignments" class="doc_header"><code>viterbi_alignments</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L141" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>viterbi_alignments</code>(<strong><code>logits</code></strong>, <strong><code>targets</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>target_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="soft_alignments" class="doc_header"><code>soft_alignments</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L146" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>soft_alignments</code>(<strong><code>logits</code></strong>, <strong><code>targets</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>target_lengths</code></strong>, <strong><code>beta</code></strong>=<em><code>1.0</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">betas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.</span><span class="p">]</span>
<span class="n">alignments</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;beta=</span><span class="si">{:.1f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">beta</span><span class="p">):</span> <span class="n">to_np</span><span class="p">(</span><span class="n">soft_alignments</span><span class="p">(</span><span class="o">*</span><span class="n">sample_inputs</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">))</span> <span class="k">for</span> <span class="n">beta</span> <span class="ow">in</span> <span class="n">betas</span><span class="p">}</span>
<span class="n">alignments</span><span class="p">[</span><span class="s1">&#39;viterbi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">to_np</span><span class="p">(</span><span class="n">viterbi_alignments</span><span class="p">(</span><span class="o">*</span><span class="n">sample_inputs</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">data</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">axs</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">alignments</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.05</span><span class="p">);</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>  
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-cyan-fg">  File </span><span class="ansi-green-fg">&#34;&lt;ipython-input-14-8f21540f804e&gt;&#34;</span><span class="ansi-cyan-fg">, line </span><span class="ansi-green-fg">5</span>
<span class="ansi-red-fg">    fig, axs = plt.subplots(2, 2, figsize=(15, 8))</span>
    ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> invalid syntax
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Direct-loss-calculation">Direct loss calculation<a class="anchor-link" href="#Direct-loss-calculation"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Just for fun, here's a numerically unstable direct space calculation of the loss. We can try to fix the numerical instability (by pre-conditioning the transition probs and normalising the fwd_bwd state at regular intervals) if we need this at a later point.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="loss_direct_cupy" class="doc_header"><code>loss_direct_cupy</code><a href="https://github.com/davidcpage/seqdist/tree/master/seqdist/ctc.py#L169" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>loss_direct_cupy</code>(<strong><code>logits</code></strong>, <strong><code>targets</code></strong>, <strong><code>input_lengths</code></strong>, <strong><code>target_lengths</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here's a mini test example which is short enough not to suffer from too much numerical instability (at least in double precision!)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span> <span class="o">=</span> <span class="n">generate_sample_inputs</span><span class="p">(</span><span class="n">T_min</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">L_min</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">L_max</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">fwds</span><span class="p">,</span> <span class="n">bwds</span> <span class="o">=</span> <span class="n">compare_fwd_bwd</span><span class="p">(</span><span class="n">float64</span><span class="p">(</span><span class="n">loss_pytorch</span><span class="p">),</span> <span class="n">float64</span><span class="p">(</span><span class="n">loss_direct_cupy</span><span class="p">),</span> <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fwd diff: 0.00e+00
bwd diff: 2.35e-08
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

